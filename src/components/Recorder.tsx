'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import type { TranscriptSegment } from '@/types';
import { DashScopeASRClient } from '@/lib/services/dashscope-asr-service';
import { TranscriptPreviewPanel } from './TranscriptPreviewPanel';
import { TranscriptEnhanceManager, type EnhancedTranscriptSegment } from '@/lib/services/transcript-enhancer';

interface RecorderProps {
  onRecordingStart?: (sessionId: string) => void;
  onRecordingStop?: (audioBlob?: Blob) => void;
  onTranscriptUpdate?: (segments: TranscriptSegment[]) => void;
  onAnchorMark?: (timestamp: number) => void;
  onTranscribing?: (isTranscribing: boolean) => void;
  disabled?: boolean;
}

type RecorderStatus = 'idle' | 'recording' | 'paused' | 'stopped' | 'transcribing';
type ServiceStatus = 'checking' | 'available' | 'unavailable' | 'asr-ready';
type TranscribeMode = 'batch' | 'streaming';

export function Recorder({
  onRecordingStart,
  onRecordingStop,
  onTranscriptUpdate,
  onAnchorMark,
  onTranscribing,
  disabled = false,
}: RecorderProps) {
  const [status, setStatus] = useState<RecorderStatus>('idle');
  const [elapsedMs, setElapsedMs] = useState(0);
  const [level, setLevel] = useState(0);
  const [transcript, setTranscript] = useState<TranscriptSegment[]>([]);
  const [interimText, setInterimText] = useState('');
  const [error, setError] = useState<string | null>(null);
  const [serviceStatus, setServiceStatus] = useState<ServiceStatus>('checking');
  const [transcribeProgress, setTranscribeProgress] = useState<string>('');
  const [transcribeMode, setTranscribeMode] = useState<TranscribeMode>('streaming');
  const [streamingAvailable, setStreamingAvailable] = useState(true);
  const [apiKey, setApiKey] = useState<string>('');
  const [wsModel, setWsModel] = useState<string>('qwen-asr-realtime-v1');
  const [wsSampleRate, setWsSampleRate] = useState<number>(16000);
  const [anchorCount, setAnchorCount] = useState(0);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationIdRef = useRef<number | null>(null);
  const startTimeRef = useRef<number>(0);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const sessionIdRef = useRef<string>('');
  const lastAnchorTimeRef = useRef<number>(0);
  const audioChunksRef = useRef<Blob[]>([]);
  const asrClientRef = useRef<DashScopeASRClient | null>(null);
  const transcriptRef = useRef<TranscriptSegment[]>([]);
  const pcmProcessorRef = useRef<ScriptProcessorNode | null>(null);
  
  // ASR åå¤„ç†å¢å¼ºç®¡ç†å™¨
  const enhanceManagerRef = useRef<TranscriptEnhanceManager | null>(null);
  const [enhancedSegments, setEnhancedSegments] = useState<Map<string, EnhancedTranscriptSegment>>(new Map());
  const [enhanceStats, setEnhanceStats] = useState({ enhanced: 0, total: 0, isEnhancing: false });

  // VAD æ£€æµ‹çŠ¶æ€
  const vadStateRef = useRef({
    isSpeaking: false,           // å½“å‰æ˜¯å¦åœ¨è¯´è¯
    speechStartMs: 0,            // è¯­éŸ³å¼€å§‹æ—¶é—´ (elapsedMs)
    silenceStartMs: 0,           // é™éŸ³å¼€å§‹æ—¶é—´ (elapsedMs)
  });

  // VAD é…ç½®å¸¸é‡
  const VAD_CONFIG = {
    energyThreshold: 0.08,       // èƒ½é‡é˜ˆå€¼ (0-1)ï¼Œæ ¹æ®ç¯å¢ƒå™ªéŸ³è°ƒæ•´
    silenceDuration: 600,        // é™éŸ³åˆ¤å®šæ—¶é•¿ (æ¯«ç§’)ï¼Œä¸ç™¾ç‚¼ server_vad å¯¹é½
    minSpeechDuration: 200,      // æœ€å°è¯­éŸ³æ—¶é•¿ (æ¯«ç§’)ï¼Œè¿‡æ»¤è¯¯è§¦å‘
  };

  // è·å– API Key å¹¶æ£€æŸ¥æœåŠ¡çŠ¶æ€
  useEffect(() => {
    const fetchConfig = async () => {
      try {
        const response = await fetch('/api/asr-config');
        if (response.ok) {
          const config = await response.json();
          setApiKey(config.apiKey);
          if (config.model) setWsModel(config.model);
          if (config.sampleRate) setWsSampleRate(config.sampleRate);
          setStreamingAvailable(true);
          setServiceStatus('available');
        } else {
          setStreamingAvailable(false);
          setServiceStatus('unavailable');
        }
      } catch {
        setStreamingAvailable(false);
        setServiceStatus('unavailable');
      }
    };
    fetchConfig();
  }, []);

  // æ ¼å¼åŒ–æ—¶é—´
  const formatTime = (ms: number) => {
    const seconds = Math.floor(ms / 1000);
    const minutes = Math.floor(seconds / 60);
    const hours = Math.floor(minutes / 60);
    const pad = (n: number) => n.toString().padStart(2, '0');
    
    if (hours > 0) {
      return `${pad(hours)}:${pad(minutes % 60)}:${pad(seconds % 60)}`;
    }
    return `${pad(minutes)}:${pad(seconds % 60)}`;
  };

  // å¼€å§‹å½•éŸ³
  const startRecording = async () => {
    try {
      setError(null);
      audioChunksRef.current = [];
      setTranscript([]);
      transcriptRef.current = [];
      setInterimText('');
      setAnchorCount(0);
      
      // åˆå§‹åŒ– ASR åå¤„ç†å¢å¼ºç®¡ç†å™¨
      setEnhancedSegments(new Map());
      setEnhanceStats({ enhanced: 0, total: 0, isEnhancing: false });
      enhanceManagerRef.current = new TranscriptEnhanceManager({
        minBatchSize: 1,          // æœ€å°‘ 1 å¥å°±å¯ä»¥è§¦å‘ä¼˜åŒ–ï¼ˆé™ä½é—¨æ§›æ–¹ä¾¿æµ‹è¯•ï¼‰
        silenceThreshold: 3000,   // 3 ç§’é™éŸ³è§¦å‘ä¼˜åŒ–
        model: 'qwen3-max',       // ä½¿ç”¨å¯ç”¨çš„ qwen3-max æ¨¡å‹
        onEnhanced: (segments) => {
          // ä¼˜åŒ–å®Œæˆå›è°ƒï¼šæ›´æ–°å¢å¼ºåçš„æ–‡æœ¬
          console.log('[Recorder] Enhanced callback received:', segments.length, 'segments');
          console.log('[Recorder] Enhanced segments:', segments.map(s => ({ id: s.id, status: s.enhanceStatus, text: s.text?.slice(0, 30) })));
          setEnhancedSegments(prev => {
            const newMap = new Map(prev);
            for (const seg of segments) {
              newMap.set(seg.id, seg);
            }
            return newMap;
          });
          setEnhanceStats(prev => ({
            ...prev,
            enhanced: prev.enhanced + segments.filter(s => s.enhanceStatus === 'enhanced').length,
            isEnhancing: false,
          }));
        },
      });
      console.log('[Recorder] TranscriptEnhanceManager initialized');

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
        },
      });

      // ä¸å¼ºåˆ¶æŒ‡å®šé‡‡æ ·ç‡ï¼Œè®© AudioContext è‡ªåŠ¨åŒ¹é…è®¾å¤‡
      // æŸäº›è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºï¼‰ä¸æ”¯æŒæŒ‡å®šé‡‡æ ·ç‡ï¼Œä¼šå¯¼è‡´ createMediaStreamSource æŠ¥é”™
      audioContextRef.current = new AudioContext();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      const actualSampleRate = audioContextRef.current.sampleRate;
      console.log('[Recorder] AudioContext sampleRate:', actualSampleRate);
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      source.connect(analyserRef.current);

      const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
      
      // é‡è¦ï¼šæå‰åˆå§‹åŒ– startTimeRefï¼Œç¡®ä¿ VAD æ—¶é—´æˆ³åŸºå‡†æ­£ç¡®
      // æ­¤æ—¶å½•éŸ³å®é™…ä¸Šå·²å¼€å§‹å‡†å¤‡ï¼Œä¸ MediaRecorder.start() å‡ ä¹åŒæ—¶
      startTimeRef.current = Date.now();
      
      // é‡ç½® VAD çŠ¶æ€
      vadStateRef.current = {
        isSpeaking: false,
        speechStartMs: 0,
        silenceStartMs: 0,
      };
      
      const checkLevel = () => {
        if (!analyserRef.current) return;
        analyserRef.current.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
        const normalizedLevel = average / 255;
        setLevel(normalizedLevel);

        // VAD èƒ½é‡æ£€æµ‹é€»è¾‘
        // ç¡®ä¿ startTimeRef å·²åˆå§‹åŒ–ï¼ˆ> 0ï¼‰
        if (startTimeRef.current > 0) {
          const currentElapsedMs = Date.now() - startTimeRef.current;
          const vadState = vadStateRef.current;
          
          if (normalizedLevel > VAD_CONFIG.energyThreshold) {
            // æ£€æµ‹åˆ°å£°éŸ³
            if (!vadState.isSpeaking) {
              // è¯­éŸ³å¼€å§‹ - ç«‹å³å‘é€å¼€å§‹æ—¶é—´æˆ³ç»™åç«¯
              vadState.isSpeaking = true;
              vadState.speechStartMs = currentElapsedMs;
              vadState.silenceStartMs = 0;
              console.log('[VAD] Speech started at', vadState.speechStartMs, 'ms, level:', normalizedLevel.toFixed(3));
              
              // å‘é€ speech-start äº‹ä»¶
              if (asrClientRef.current?.isConnected()) {
                asrClientRef.current.sendVADEvent('start', vadState.speechStartMs);
              }
            }
            // é‡ç½®é™éŸ³è®¡æ—¶
            vadState.silenceStartMs = 0;
          } else {
            // é™éŸ³çŠ¶æ€
            if (vadState.isSpeaking) {
              // æ­£åœ¨è¯´è¯ä½†æ£€æµ‹åˆ°é™éŸ³
              if (vadState.silenceStartMs === 0) {
                // å¼€å§‹è®¡æ—¶é™éŸ³
                vadState.silenceStartMs = currentElapsedMs;
              } else {
                // æ£€æŸ¥é™éŸ³æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                const silenceDuration = currentElapsedMs - vadState.silenceStartMs;
                if (silenceDuration >= VAD_CONFIG.silenceDuration) {
                  // è¯­éŸ³ç»“æŸ
                  const speechDuration = vadState.silenceStartMs - vadState.speechStartMs;
                  if (speechDuration >= VAD_CONFIG.minSpeechDuration) {
                    // æœ‰æ•ˆè¯­éŸ³æ®µç»“æŸï¼Œå‘é€ç»“æŸæ—¶é—´æˆ³
                    console.log('[VAD] Speech ended:', vadState.speechStartMs, '-', vadState.silenceStartMs, 'ms, duration:', speechDuration, 'ms');
                    if (asrClientRef.current?.isConnected()) {
                      asrClientRef.current.sendVADEvent('end', vadState.silenceStartMs);
                    }
                  }
                  vadState.isSpeaking = false;
                  vadState.speechStartMs = 0;
                  vadState.silenceStartMs = 0;
                }
              }
            }
          }
        }

        animationIdRef.current = requestAnimationFrame(checkLevel);
      };
      checkLevel();

      sessionIdRef.current = `session-${Date.now()}`;

      // æµå¼æ¨¡å¼
      if (transcribeMode === 'streaming' && streamingAvailable && apiKey) {
        asrClientRef.current = new DashScopeASRClient(apiKey, {
          onSentence: (sentence) => {
            const segment: TranscriptSegment = {
              id: sentence.id,
              text: sentence.text,
              startMs: sentence.beginTime,
              endMs: sentence.endTime || sentence.beginTime,
              confidence: 0.95,
              isFinal: true,
            };
            transcriptRef.current = [...transcriptRef.current, segment];
            setTranscript(transcriptRef.current);
            onTranscriptUpdate?.(transcriptRef.current);
            
            // å°†æ–°å¥å­æ·»åŠ åˆ°å¢å¼ºç®¡ç†å™¨ï¼Œç­‰å¾…æ‰¹é‡ä¼˜åŒ–
            if (enhanceManagerRef.current) {
              console.log('[Recorder] Adding segment to enhance manager:', segment.id, segment.text?.slice(0, 30));
              enhanceManagerRef.current.addSegment(segment);
              setEnhanceStats(prev => ({ ...prev, total: prev.total + 1 }));
            }
          },
          onInterim: (text) => {
            setInterimText(text);
            // æ›´æ–°æ´»åŠ¨æ—¶é—´ï¼Œç”¨äºé™éŸ³æ£€æµ‹
            if (enhanceManagerRef.current) {
              enhanceManagerRef.current.updateActivity();
            }
          },
          onError: (err) => setError(err),
          onStatusChange: (newStatus) => {
            if (newStatus === 'transcribing') setServiceStatus('available');
          },
        }, {
          model: wsModel,
          sampleRate: wsSampleRate,
          format: 'pcm',
        });
        
        const started = await asrClientRef.current.start();
        if (!started) {
          asrClientRef.current = null;
        } else {
          const bufferSize = 4096;
          pcmProcessorRef.current = audioContextRef.current.createScriptProcessor(bufferSize, 1, 1);
          
          // é‡é‡‡æ ·å‡½æ•°ï¼šå°†è®¾å¤‡é‡‡æ ·ç‡è½¬æ¢ä¸ºç›®æ ‡é‡‡æ ·ç‡ (16000Hz)
          const resample = (inputData: Float32Array, fromRate: number, toRate: number): Float32Array => {
            if (fromRate === toRate) return inputData;
            const ratio = fromRate / toRate;
            const newLength = Math.round(inputData.length / ratio);
            const result = new Float32Array(newLength);
            for (let i = 0; i < newLength; i++) {
              const srcIndex = i * ratio;
              const srcIndexFloor = Math.floor(srcIndex);
              const srcIndexCeil = Math.min(srcIndexFloor + 1, inputData.length - 1);
              const t = srcIndex - srcIndexFloor;
              // çº¿æ€§æ’å€¼
              result[i] = inputData[srcIndexFloor] * (1 - t) + inputData[srcIndexCeil] * t;
            }
            return result;
          };
          
          pcmProcessorRef.current.onaudioprocess = (e) => {
            if (asrClientRef.current?.isConnected()) {
              const inputData = e.inputBuffer.getChannelData(0);
              // é‡é‡‡æ ·åˆ° 16000Hz
              const resampledData = resample(inputData, actualSampleRate, wsSampleRate);
              const pcmData = new Int16Array(resampledData.length);
              for (let i = 0; i < resampledData.length; i++) {
                pcmData[i] = Math.max(-32768, Math.min(32767, Math.floor(resampledData[i] * 32768)));
              }
              asrClientRef.current.sendAudio(pcmData.buffer);
            }
          };
          
          source.connect(pcmProcessorRef.current);
          pcmProcessorRef.current.connect(audioContextRef.current.destination);
        }
      }

      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType,
        audioBitsPerSecond: 64000,
      });

      mediaRecorder.ondataavailable = async (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.start(1000);
      mediaRecorderRef.current = mediaRecorder;

      // startTimeRef.current å·²åœ¨ checkLevel åˆå§‹åŒ–å‰è®¾ç½®ï¼Œæ­¤å¤„ä»…å¯åŠ¨è®¡æ—¶å™¨
      timerRef.current = setInterval(() => {
        setElapsedMs(Date.now() - startTimeRef.current);
      }, 100);

      setStatus('recording');
      onRecordingStart?.(sessionIdRef.current);

    } catch (err) {
      setError(err instanceof Error ? err.message : 'å½•éŸ³å¯åŠ¨å¤±è´¥');
    }
  };

  // æš‚åœå½•éŸ³
  const pauseRecording = () => {
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.pause();
      if (timerRef.current) clearInterval(timerRef.current);
      setStatus('paused');
    }
  };

  // ç»§ç»­å½•éŸ³
  const resumeRecording = () => {
    if (mediaRecorderRef.current?.state === 'paused') {
      mediaRecorderRef.current.resume();
      const pausedTime = elapsedMs;
      startTimeRef.current = Date.now() - pausedTime;
      timerRef.current = setInterval(() => {
        setElapsedMs(Date.now() - startTimeRef.current);
      }, 100);
      
      // é‡ç½® VAD çŠ¶æ€ï¼Œé¿å…æš‚åœæœŸé—´çš„é™éŸ³è¢«è¯¯åˆ¤
      vadStateRef.current = {
        isSpeaking: false,
        speechStartMs: 0,
        silenceStartMs: 0,
      };
      
      setStatus('recording');
    }
  };

  // åœæ­¢å½•éŸ³
  const stopRecording = async () => {
    // é¦–å…ˆåœæ­¢ ASRï¼Œé˜»æ­¢ç»§ç»­å¤„ç†éŸ³é¢‘
    if (asrClientRef.current) {
      await asrClientRef.current.stop();
      asrClientRef.current = null;
    }

    // æ–­å¼€éŸ³é¢‘å¤„ç†å™¨
    if (pcmProcessorRef.current) {
      pcmProcessorRef.current.disconnect();
      pcmProcessorRef.current = null;
    }

    if (animationIdRef.current) {
      cancelAnimationFrame(animationIdRef.current);
      animationIdRef.current = null;
    }

    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });

    mediaRecorderRef.current = null;
    analyserRef.current = null;
    setLevel(0);
    setInterimText('');

    // è§¦å‘æœ€ç»ˆçš„ ASR åå¤„ç†ä¼˜åŒ–
    if (enhanceManagerRef.current && transcribeMode === 'streaming') {
      setEnhanceStats(prev => ({ ...prev, isEnhancing: true }));
      console.log('[Recorder] Finalizing transcript enhancement...');
      try {
        await enhanceManagerRef.current.finalize();
      } catch (err) {
        console.error('[Recorder] Enhancement finalize error:', err);
      }
      // æ¸…ç†å¢å¼ºç®¡ç†å™¨
      enhanceManagerRef.current.dispose();
      enhanceManagerRef.current = null;
    }

    if (transcribeMode === 'batch' && audioBlob.size > 0) {
      await transcribeWithQwenASR(audioBlob);
    } else {
      if (transcribeMode === 'streaming' && transcriptRef.current.length > 0) {
        const enhancedCount = enhanceStats.enhanced;
        const totalCount = transcriptRef.current.length;
        const enhanceInfo = enhancedCount > 0 ? `ï¼Œå·²ä¼˜åŒ– ${enhancedCount} å¥` : '';
        setTranscribeProgress(`è½¬å½•å®Œæˆï¼Œå…± ${totalCount} å¥${enhanceInfo}`);
        onTranscriptUpdate?.(transcriptRef.current);
      }
      setStatus('stopped');
      onRecordingStop?.(audioBlob);
    }
  };

  // éæµå¼è½¬å½•
  const transcribeWithQwenASR = async (audioBlob: Blob) => {
    setStatus('transcribing');
    setTranscribeProgress('æ­£åœ¨è½¬å½•éŸ³é¢‘...');
    onTranscribing?.(true);

    try {
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');

      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData,
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || 'è½¬å½•å¤±è´¥');
      }

      const data = await response.json();

      if (data.success && data.segments) {
        const segments: TranscriptSegment[] = data.segments.map((seg: {
          id: string;
          text: string;
          startMs: number;
          endMs: number;
          confidence?: number;
        }) => ({
          id: seg.id,
          text: seg.text,
          startMs: seg.startMs,
          endMs: seg.endMs,
          confidence: seg.confidence || 0.95,
          isFinal: true,
        }));

        setTranscript(segments);
        onTranscriptUpdate?.(segments);
        setTranscribeProgress(`è½¬å½•å®Œæˆï¼Œå…± ${segments.length} å¥`);
      } else {
        setTranscribeProgress('è½¬å½•å®Œæˆï¼Œä½†æœªè·å–åˆ°æ–‡æœ¬');
      }
    } catch (err) {
      setError(err instanceof Error ? err.message : 'è½¬å½•å¤±è´¥');
      setTranscribeProgress('');
    } finally {
      onTranscribing?.(false);
      setStatus('stopped');
      onRecordingStop?.(audioBlob);
    }
  };

  // æ ‡è®°æ–­ç‚¹
  const markAnchor = useCallback(() => {
    if (status !== 'recording') return;
    
    const timestamp = elapsedMs;
    lastAnchorTimeRef.current = timestamp;
    onAnchorMark?.(timestamp);
    setAnchorCount(prev => prev + 1);
  }, [status, elapsedMs, onAnchorMark]);

  // æ¸…ç†
  useEffect(() => {
    return () => {
      if (animationIdRef.current) cancelAnimationFrame(animationIdRef.current);
      if (timerRef.current) clearInterval(timerRef.current);
      if (pcmProcessorRef.current) pcmProcessorRef.current.disconnect();
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop();
        mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
      }
      if (audioContextRef.current) audioContextRef.current.close();
      if (asrClientRef.current) asrClientRef.current.stop();
      if (enhanceManagerRef.current) enhanceManagerRef.current.dispose();
    };
  }, []);


  const isRecording = status === 'recording';
  const isTranscribing = status === 'transcribing';
  const isStopped = status === 'stopped';
  const isIdle = status === 'idle';

  // åˆå¹¶åŸå§‹è½¬å½•å’Œå¢å¼ºåçš„æ–‡æœ¬ï¼Œä¼˜å…ˆæ˜¾ç¤ºå¢å¼ºç‰ˆæœ¬
  const displayTranscript = transcript.map(seg => {
    const enhanced = enhancedSegments.get(seg.id);
    if (enhanced && enhanced.enhanceStatus === 'enhanced' && enhanced.text !== seg.text) {
      return {
        ...seg,
        text: enhanced.text,
        originalText: seg.text, // ä¿ç•™åŸå§‹æ–‡æœ¬ä»¥ä¾¿å¯¹æ¯”
      };
    }
    return seg;
  });

  // ===== é—²ç½®çŠ¶æ€ï¼šæ˜¾ç¤ºå¼€å§‹å½•éŸ³ç•Œé¢ =====
  if (isIdle) {
    return (
      <div className="card p-8 animate-fade-in">
        {/* é¡¶éƒ¨çŠ¶æ€æ  */}
        <div className="flex items-center justify-between mb-8">
          <div className="flex items-center gap-2">
            <div className={`w-2 h-2 rounded-full transition-colors ${
              serviceStatus === 'checking' ? 'bg-sunflower animate-pulse' :
              serviceStatus === 'available' ? 'bg-mint' :
              'bg-gray-300'
            }`} />
            <span className="text-xs text-gray-500">
              {serviceStatus === 'checking' ? 'è¿æ¥ä¸­...' :
               serviceStatus === 'available' ? 'å®æ—¶è½¬å½•å°±ç»ª' :
               'æœ¬åœ°æ¨¡å¼'}
            </span>
          </div>
          
          {/* æ¨¡å¼åˆ‡æ¢ */}
          <div className="flex flex-col items-end gap-1">
            <div className="flex items-center gap-1 p-1 bg-gray-100 rounded-lg">
              <button
                onClick={() => setTranscribeMode('streaming')}
                disabled={!streamingAvailable}
                className={`px-3 py-1.5 text-xs rounded-md transition-all ${
                  transcribeMode === 'streaming' 
                    ? 'bg-white text-amber-600 shadow-sm' 
                    : 'text-gray-500 hover:text-gray-700'
                } ${!streamingAvailable ? 'opacity-50 cursor-not-allowed' : ''}`}
              >
                âš¡ è¾¹å½•è¾¹è½¬
              </button>
              <button
                onClick={() => setTranscribeMode('batch')}
                className={`px-3 py-1.5 text-xs rounded-md transition-all ${
                  transcribeMode === 'batch' 
                    ? 'bg-white text-accent-600 shadow-sm' 
                    : 'text-gray-500 hover:text-gray-700'
                }`}
              >
                ğŸ¯ å½•å®Œè½¬è¯‘
              </button>
            </div>
            <span className="text-[10px] text-gray-400">
              {transcribeMode === 'streaming' ? 'è¾¹å¬è¾¹çœ‹æ–‡å­—ï¼Œé€‚åˆä¸Šè¯¾' : 'å½•å®Œå†è½¬ï¼Œæ›´å‡†ç¡®'}
            </span>
          </div>
        </div>

        {/* é”™è¯¯æç¤º */}
        {error && (
          <div className="mb-6 p-4 bg-coral-50 border border-coral-100 rounded-xl text-coral-600 text-sm animate-slide-up">
            <div className="flex items-center gap-2">
              <span>âš ï¸</span>
              <span>{error}</span>
            </div>
          </div>
        )}

        {/* å¼€å§‹å½•éŸ³åŒºåŸŸ */}
        <div className="flex flex-col items-center py-12">
          <div className="text-6xl font-mono font-bold text-gray-200 mb-4">
            00:00
          </div>
          <p className="text-sm text-gray-400 mb-8">ç‚¹å‡»å¼€å§‹å½•åˆ¶è¯¾å ‚</p>
          <button
            onClick={startRecording}
            disabled={disabled}
            className="record-btn"
            aria-label="å¼€å§‹å½•éŸ³"
            data-onboarding="record-button"
          >
            <svg className="w-8 h-8 text-white" fill="currentColor" viewBox="0 0 24 24">
              <circle cx="12" cy="12" r="6" />
            </svg>
          </button>
        </div>
      </div>
    );
  }

  // ===== è½¬å½•ä¸­çŠ¶æ€ =====
  if (isTranscribing) {
    return (
      <div className="card p-8 animate-fade-in">
        <div className="flex flex-col items-center py-12">
          <div className="w-20 h-20 rounded-full bg-sunflower-100 flex items-center justify-center mb-6">
            <div className="w-8 h-8 border-3 border-sunflower border-t-transparent rounded-full animate-spin" />
          </div>
          <div className="text-lg font-medium text-gray-700 mb-2">æ­£åœ¨è½¬å½•</div>
          <p className="text-sm text-gray-500">{transcribeProgress || 'è¯·ç¨å€™...'}</p>
        </div>
      </div>
    );
  }

  // ===== åœæ­¢çŠ¶æ€ï¼šæ˜¾ç¤ºå®Œæˆç•Œé¢ =====
  if (isStopped) {
    return (
      <div className="card p-8 animate-fade-in">
        {/* å®Œæˆæç¤º */}
        {transcribeProgress && (
          <div className="mb-6 p-4 bg-mint-50 border border-mint-200 rounded-xl animate-scale-in">
            <div className="flex items-center gap-2 text-mint-700">
              <span className="text-lg">âœ…</span>
              <span className="text-sm font-medium">{transcribeProgress}</span>
            </div>
          </div>
        )}

        {/* è½¬å½•ç»“æœé¢„è§ˆ - ä½¿ç”¨å¢å¼ºåçš„æ–‡æœ¬ */}
        <TranscriptPreviewPanel
          transcript={displayTranscript}
          interimText=""
          isRecording={false}
          transcribeMode={transcribeMode}
          collapsedCount={10}
          formatTime={formatTime}
          defaultExpanded={true}
        />

        {/* æ“ä½œæŒ‰é’® */}
        <div className="mt-6 flex justify-center">
          <button
            onClick={() => {
              setStatus('idle');
              setElapsedMs(0);
              setTranscript([]);
              setInterimText('');
              setTranscribeProgress('');
              setAnchorCount(0);
              setEnhancedSegments(new Map());
              setEnhanceStats({ enhanced: 0, total: 0, isEnhancing: false });
              audioChunksRef.current = [];
            }}
            className="btn btn-primary px-8 py-3"
          >
            <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 24 24">
              <path d="M17.65 6.35A7.958 7.958 0 0012 4c-4.42 0-7.99 3.58-7.99 8s3.57 8 7.99 8c3.73 0 6.84-2.55 7.73-6h-2.08A5.99 5.99 0 0112 18c-3.31 0-6-2.69-6-6s2.69-6 6-6c1.66 0 3.14.69 4.22 1.78L13 11h7V4l-2.35 2.35z" />
            </svg>
            å¼€å§‹æ–°å½•éŸ³
          </button>
        </div>
      </div>
    );
  }

  // ===== å½•éŸ³æ´»åŠ¨çŠ¶æ€ï¼šæ²‰æµ¸å¼è½¬å½•å¸ƒå±€ =====
  return (
    <div className="flex flex-col h-full bg-white rounded-2xl shadow-sm border border-gray-100 overflow-hidden animate-fade-in">
      {/* ===== æç®€é¡¶æ  (ç§»åŠ¨ç«¯ 84px / æ¡Œé¢ç«¯ 60px) ===== */}
      <div className="flex-shrink-0 h-[84px] sm:h-[60px] px-4 flex items-center justify-between border-b border-gray-100 bg-gradient-to-r from-gray-50 to-white">
        {/* å·¦ä¾§ï¼šçŠ¶æ€å’Œæ—¶é—´ */}
        <div className="flex items-center gap-3 sm:gap-3">
          {/* å½•éŸ³çŠ¶æ€æŒ‡ç¤ºå™¨ */}
          <div className="flex items-center gap-2 sm:gap-2">
            <div className={`w-3.5 h-3.5 sm:w-2.5 sm:h-2.5 rounded-full ${isRecording ? 'bg-coral animate-pulse' : 'bg-sunflower-500'}`} />
            <span className={`text-lg sm:text-sm font-medium ${isRecording ? 'text-coral' : 'text-sunflower-600'}`}>
              {isRecording ? 'å½•éŸ³ä¸­' : 'å·²æš‚åœ'}
            </span>
          </div>
          
          {/* æ—¶é—´æ˜¾ç¤º - ç§»åŠ¨ç«¯æ›´å¤§ (æ”¾å¤§1.3å€) */}
          <div className={`font-mono text-4xl sm:text-2xl font-semibold tabular-nums ${isRecording ? 'text-gray-800' : 'text-gray-500'}`}>
            {formatTime(elapsedMs)}
          </div>
          
          {/* éŸ³é‡æŒ‡ç¤ºå™¨ - ä»…æ¡Œé¢ç«¯æ˜¾ç¤º */}
          {isRecording && (
            <div className="hidden sm:flex items-center gap-0.5 h-5">
              {[...Array(5)].map((_, i) => (
                <div
                  key={i}
                  className={`w-1 rounded-full transition-all duration-75 ${
                    level * 5 > i ? 'bg-mint' : 'bg-gray-200'
                  }`}
                  style={{ 
                    height: `${Math.max(4, Math.min(16, (level * 20) + Math.sin(Date.now() / 200 + i) * 2))}px`
                  }}
                />
              ))}
            </div>
          )}
        </div>

        {/* å³ä¾§ï¼šæ§åˆ¶æŒ‰é’®å’Œæ¨¡å¼æ ‡ç­¾ */}
        <div className="flex items-center gap-4 sm:gap-4">
          {/* æ¨¡å¼æ ‡ç­¾ */}
          <span className="text-sm sm:text-xs text-gray-400 hidden sm:inline">
            {transcribeMode === 'streaming' ? 'è¾¹å½•è¾¹è½¬' : 'å½•å®Œè½¬è¯‘'}
          </span>
          
          {/* æ§åˆ¶æŒ‰é’®ç»„ - ç§»åŠ¨ç«¯å¤§è§¦æ‘¸ç›®æ ‡ (72pxï¼Œæ”¾å¤§1.3å€)ï¼Œæ¡Œé¢ç«¯ (48px) */}
          <div className="flex items-center gap-4 sm:gap-2">
            {isRecording ? (
              <button
                onClick={pauseRecording}
                className="w-[72px] h-[72px] sm:w-[48px] sm:h-[48px] rounded-full bg-sunflower-100 text-sunflower-700 flex items-center justify-center hover:bg-sunflower-200 transition-all active:scale-95 shadow-sm"
                aria-label="æš‚åœ"
              >
                <svg className="w-9 h-9 sm:w-6 sm:h-6" fill="currentColor" viewBox="0 0 24 24">
                  <rect x="6" y="4" width="4" height="16" rx="1" />
                  <rect x="14" y="4" width="4" height="16" rx="1" />
                </svg>
              </button>
            ) : (
              <button
                onClick={resumeRecording}
                className="w-[72px] h-[72px] sm:w-[48px] sm:h-[48px] rounded-full bg-mint-100 text-mint-700 flex items-center justify-center hover:bg-mint-200 transition-all active:scale-95 shadow-sm"
                aria-label="ç»§ç»­"
              >
                <svg className="w-9 h-9 sm:w-6 sm:h-6" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M8 5v14l11-7z" />
                </svg>
              </button>
            )}
            <button
              onClick={stopRecording}
              className="w-[72px] h-[72px] sm:w-[48px] sm:h-[48px] rounded-full bg-gray-100 text-gray-600 flex items-center justify-center hover:bg-gray-200 transition-all active:scale-95 shadow-sm"
              aria-label="åœæ­¢"
            >
              <svg className="w-9 h-9 sm:w-6 sm:h-6" fill="currentColor" viewBox="0 0 24 24">
                <rect x="6" y="6" width="12" height="12" rx="2" />
              </svg>
            </button>
          </div>
        </div>
      </div>

      {/* ===== é”™è¯¯æç¤ºï¼ˆå¦‚æœ‰ï¼‰ ===== */}
      {error && (
        <div className="flex-shrink-0 mx-4 mt-3 p-3 bg-coral-50 border border-coral-100 rounded-xl text-coral-600 text-sm animate-slide-up">
          <div className="flex items-center gap-2">
            <span>âš ï¸</span>
            <span>{error}</span>
          </div>
        </div>
      )}

      {/* ===== ASR åå¤„ç†ä¼˜åŒ–çŠ¶æ€æŒ‡ç¤ºå™¨ ===== */}
      {transcribeMode === 'streaming' && enhanceStats.total > 0 && (
        <div className="flex-shrink-0 mx-4 mt-2 flex items-center gap-2 text-xs text-gray-400">
          {enhanceStats.isEnhancing ? (
            <>
              <div className="w-3 h-3 border-2 border-amber-400 border-t-transparent rounded-full animate-spin" />
              <span>æ­£åœ¨ä¼˜åŒ–æ–‡æœ¬...</span>
            </>
          ) : enhanceStats.enhanced > 0 ? (
            <>
              <span className="text-mint-600">âœ¨</span>
              <span>å·²ä¼˜åŒ– {enhanceStats.enhanced}/{enhanceStats.total} å¥</span>
            </>
          ) : null}
        </div>
      )}

      {/* ===== æ²‰æµ¸å¼è½¬å½•åŒºåŸŸï¼ˆå æ®ä¸»è¦ç©ºé—´ï¼Œå¯æ»šåŠ¨ï¼‰ ===== */}
      <div className="flex-1 overflow-y-auto min-h-0">
        <TranscriptPreviewPanel
          transcript={displayTranscript}
          interimText={interimText}
          isRecording={isRecording}
          transcribeMode={transcribeMode}
          collapsedCount={999}
          formatTime={formatTime}
          defaultExpanded={true}
          immersiveMode={true}
        />
      </div>

      {/* ===== å›ºå®šåº•éƒ¨ï¼šå›°æƒ‘ç‚¹æŒ‰é’® (48px) ===== */}
      <div className="flex-shrink-0 border-t border-gray-100 bg-gradient-to-r from-white to-gray-50">
        <button
          onClick={markAnchor}
          disabled={!isRecording}
          className={`w-full h-14 flex items-center justify-center gap-3 transition-all ${
            isRecording 
              ? 'bg-gradient-to-r from-coral-500 to-warmOrange-500 text-white hover:from-coral-600 hover:to-warmOrange-600 active:scale-[0.99]' 
              : 'bg-gray-100 text-gray-400 cursor-not-allowed'
          }`}
          data-onboarding="confusion-button"
        >
          <span className="text-xl">ğŸ¯</span>
          <span className="font-medium">æ²¡å¬æ‡‚ï¼Ÿç‚¹è¿™é‡Œï¼</span>
          {anchorCount > 0 && (
            <span className={`ml-2 px-2 py-0.5 rounded-full text-xs ${
              isRecording ? 'bg-white/20' : 'bg-gray-200'
            }`}>
              å·²æ ‡è®° {anchorCount}
            </span>
          )}
        </button>
      </div>
    </div>
  );
}
