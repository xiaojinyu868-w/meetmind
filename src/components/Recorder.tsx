'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import type { TranscriptSegment } from '@/types';
import { DashScopeASRClient } from '@/lib/services/dashscope-asr-service';

interface RecorderProps {
  onRecordingStart?: (sessionId: string) => void;
  onRecordingStop?: (audioBlob?: Blob) => void;
  onTranscriptUpdate?: (segments: TranscriptSegment[]) => void;
  onAnchorMark?: (timestamp: number) => void;
  onTranscribing?: (isTranscribing: boolean) => void;
  disabled?: boolean;
}

type RecorderStatus = 'idle' | 'recording' | 'paused' | 'stopped' | 'transcribing';
type ServiceStatus = 'checking' | 'available' | 'unavailable' | 'asr-ready';
type TranscribeMode = 'batch' | 'streaming';  // batch=éæµå¼, streaming=æµå¼

export function Recorder({
  onRecordingStart,
  onRecordingStop,
  onTranscriptUpdate,
  onAnchorMark,
  onTranscribing,
  disabled = false,
}: RecorderProps) {
  const [status, setStatus] = useState<RecorderStatus>('idle');
  const [elapsedMs, setElapsedMs] = useState(0);
  const [level, setLevel] = useState(0);
  const [transcript, setTranscript] = useState<TranscriptSegment[]>([]);
  const [interimText, setInterimText] = useState('');
  const [error, setError] = useState<string | null>(null);
  const [canUndo, setCanUndo] = useState(false);
  const [serviceStatus, setServiceStatus] = useState<ServiceStatus>('checking');
  const [transcribeProgress, setTranscribeProgress] = useState<string>('');
  const [transcribeMode, setTranscribeMode] = useState<TranscribeMode>('streaming');  // é»˜è®¤æµå¼ä¼˜å…ˆ
  const [streamingAvailable, setStreamingAvailable] = useState(true);  // å¯ç”¨æµå¼
  const [apiKey, setApiKey] = useState<string>('');
  const [wsModel, setWsModel] = useState<string>('qwen-asr-realtime-v1');
  const [wsSampleRate, setWsSampleRate] = useState<number>(16000);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationIdRef = useRef<number | null>(null);
  const startTimeRef = useRef<number>(0);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const sessionIdRef = useRef<string>('');
  const lastAnchorTimeRef = useRef<number>(0);
  const audioChunksRef = useRef<Blob[]>([]);  // å­˜å‚¨å½•éŸ³æ•°æ®
  const asrClientRef = useRef<DashScopeASRClient | null>(null);  // ç™¾ç‚¼æµå¼è½¬å½•å®¢æˆ·ç«¯
  const transcriptRef = useRef<TranscriptSegment[]>([]);  // ç”¨äºæµå¼æ›´æ–°
  const pcmProcessorRef = useRef<ScriptProcessorNode | null>(null);

  // è·å– API Key å¹¶æ£€æŸ¥æœåŠ¡çŠ¶æ€
  useEffect(() => {
    const fetchConfig = async () => {
      try {
        const response = await fetch('/api/asr-config');
        if (response.ok) {
          const config = await response.json();
          setApiKey(config.apiKey);
          if (config.model) setWsModel(config.model);
          if (config.sampleRate) setWsSampleRate(config.sampleRate);
          setStreamingAvailable(true);
          setServiceStatus('available');
          console.log('[Recorder] ASR service ready (streaming preferred)', config.model || 'default');
        } else {
          setStreamingAvailable(false);
          setServiceStatus('unavailable');
          console.error('[Recorder] Failed to get ASR config');
        }
      } catch (err) {
        setStreamingAvailable(false);
        setServiceStatus('unavailable');
        console.error('[Recorder] Error fetching ASR config:', err);
      }
    };
    fetchConfig();
  }, []);

  // æ ¼å¼åŒ–æ—¶é—´
  const formatTime = (ms: number) => {
    const seconds = Math.floor(ms / 1000);
    const minutes = Math.floor(seconds / 60);
    const hours = Math.floor(minutes / 60);
    const pad = (n: number) => n.toString().padStart(2, '0');
    
    if (hours > 0) {
      return `${pad(hours)}:${pad(minutes % 60)}:${pad(seconds % 60)}`;
    }
    return `${pad(minutes)}:${pad(seconds % 60)}`;
  };

  // å¼€å§‹å½•éŸ³
  const startRecording = async () => {
    try {
      setError(null);
      audioChunksRef.current = [];  // æ¸…ç©ºä¹‹å‰çš„å½•éŸ³æ•°æ®
      setTranscript([]);
      transcriptRef.current = [];
      setInterimText('');

      // è¯·æ±‚éº¦å…‹é£æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
        },
      });

      // åˆ›å»ºéŸ³é¢‘åˆ†æå™¨
      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      const source = audioContextRef.current.createMediaStreamSource(stream);
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      source.connect(analyserRef.current);

      // éŸ³é‡ç›‘æµ‹
      const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
      const checkLevel = () => {
        if (!analyserRef.current) return;
        analyserRef.current.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
        setLevel(average / 255);
        animationIdRef.current = requestAnimationFrame(checkLevel);
      };
      checkLevel();

      // ç”Ÿæˆä¼šè¯ ID
      sessionIdRef.current = `session-${Date.now()}`;

      // æµå¼æ¨¡å¼ï¼šåˆå§‹åŒ–ç™¾ç‚¼ ASR å®¢æˆ·ç«¯
      if (transcribeMode === 'streaming' && streamingAvailable && apiKey) {
        asrClientRef.current = new DashScopeASRClient(apiKey, {
          onSentence: (sentence) => {
            const segment: TranscriptSegment = {
              id: sentence.id,
              text: sentence.text,
              startMs: sentence.beginTime,
              endMs: sentence.endTime || sentence.beginTime,
              confidence: 0.95,
              isFinal: true,
            };
            transcriptRef.current = [...transcriptRef.current, segment];
            setTranscript(transcriptRef.current);
            onTranscriptUpdate?.(transcriptRef.current);
          },
          onInterim: (text) => {
            setInterimText(text);
          },
          onError: (err) => {
            console.error('[Streaming] Error:', err);
            setError(err);
          },
          onStatusChange: (newStatus) => {
            console.log('[Streaming] Status:', newStatus);
            if (newStatus === 'transcribing') {
              setServiceStatus('available');
            }
          },
        }, {
          model: wsModel,
          sampleRate: wsSampleRate,
          format: 'pcm',
        });
        
        // å¯åŠ¨æµå¼è½¬å½•
        const started = await asrClientRef.current.start();
        if (!started) {
          console.warn('[Recorder] Failed to start streaming ASR, falling back to batch mode');
          asrClientRef.current = null;
        } else {
          // åˆ›å»º PCM å¤„ç†å™¨å‘é€éŸ³é¢‘æ•°æ®
          const bufferSize = 4096;
          pcmProcessorRef.current = audioContextRef.current.createScriptProcessor(bufferSize, 1, 1);
          
          pcmProcessorRef.current.onaudioprocess = (e) => {
            if (asrClientRef.current?.isConnected()) {
              const inputData = e.inputBuffer.getChannelData(0);
              // è½¬æ¢ä¸º 16-bit PCM
              const pcmData = new Int16Array(inputData.length);
              for (let i = 0; i < inputData.length; i++) {
                pcmData[i] = Math.max(-32768, Math.min(32767, Math.floor(inputData[i] * 32768)));
              }
              asrClientRef.current.sendAudio(pcmData.buffer);
            }
          };
          
          source.connect(pcmProcessorRef.current);
          pcmProcessorRef.current.connect(audioContextRef.current.destination);
        }
      }

      // åˆ›å»º MediaRecorderï¼ˆç”¨äºä¿å­˜å®Œæ•´å½•éŸ³ï¼‰
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType,
        audioBitsPerSecond: 64000,
      });

      // å¤„ç†éŸ³é¢‘æ•°æ®ï¼ˆä¿å­˜ç”¨äºéæµå¼è½¬å½•æˆ–å¤‡ä»½ï¼‰
      mediaRecorder.ondataavailable = async (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      // æ¯ç§’ä¿å­˜ä¸€æ¬¡æ•°æ®
      mediaRecorder.start(1000);
      mediaRecorderRef.current = mediaRecorder;

      // å¼€å§‹è®¡æ—¶
      startTimeRef.current = Date.now();
      timerRef.current = setInterval(() => {
        setElapsedMs(Date.now() - startTimeRef.current);
      }, 100);

      setStatus('recording');
      onRecordingStart?.(sessionIdRef.current);

    } catch (err) {
      setError(err instanceof Error ? err.message : 'å½•éŸ³å¯åŠ¨å¤±è´¥');
    }
  };

  // æš‚åœå½•éŸ³
  const pauseRecording = () => {
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.pause();
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      setStatus('paused');
    }
  };

  // ç»§ç»­å½•éŸ³
  const resumeRecording = () => {
    if (mediaRecorderRef.current?.state === 'paused') {
      mediaRecorderRef.current.resume();
      const pausedTime = elapsedMs;
      startTimeRef.current = Date.now() - pausedTime;
      timerRef.current = setInterval(() => {
        setElapsedMs(Date.now() - startTimeRef.current);
      }, 100);
      setStatus('recording');
    }
  };

  // åœæ­¢å½•éŸ³
  const stopRecording = async () => {
    // åœæ­¢åŠ¨ç”»
    if (animationIdRef.current) {
      cancelAnimationFrame(animationIdRef.current);
      animationIdRef.current = null;
    }

    // åœæ­¢è®¡æ—¶
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }

    // åœæ­¢ PCM å¤„ç†å™¨
    if (pcmProcessorRef.current) {
      pcmProcessorRef.current.disconnect();
      pcmProcessorRef.current = null;
    }

    // åœæ­¢å½•éŸ³
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
    }

    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    // åœæ­¢æµå¼è½¬å½•
    if (asrClientRef.current) {
      await asrClientRef.current.stop();
      asrClientRef.current = null;
    }

    // åˆå¹¶éŸ³é¢‘æ•°æ®
    const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
    console.log('[Recorder] Audio blob size:', audioBlob.size);

    mediaRecorderRef.current = null;
    analyserRef.current = null;
    setLevel(0);
    setInterimText('');

    // éæµå¼æ¨¡å¼ï¼šä½¿ç”¨ qwen-asr-flash è¿›è¡Œè½¬å½•
    if (transcribeMode === 'batch' && audioBlob.size > 0) {
      await transcribeWithQwenASR(audioBlob);
    } else {
      // æµå¼æ¨¡å¼å·²ç»å®æ—¶è½¬å½•å®Œæˆ
      if (transcribeMode === 'streaming' && transcriptRef.current.length > 0) {
        setTranscribeProgress(`æµå¼è½¬å½•å®Œæˆï¼Œå…± ${transcriptRef.current.length} ä¸ªå¥å­`);
        onTranscriptUpdate?.(transcriptRef.current);
      }
      setStatus('stopped');
      onRecordingStop?.(audioBlob);
    }
  };

  // ä½¿ç”¨ qwen-asr-flash è¿›è¡Œéæµå¼è½¬å½•
  const transcribeWithQwenASR = async (audioBlob: Blob) => {
    setStatus('transcribing');
    setTranscribeProgress('æ­£åœ¨è½¬å½•éŸ³é¢‘...');
    onTranscribing?.(true);

    console.log('[Recorder] Starting transcription with Qwen ASR...');
    console.log('[Recorder] Audio blob:', { size: audioBlob.size, type: audioBlob.type });

    try {
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');

      console.log('[Recorder] Sending request to /api/transcribe...');
      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData,
      });

      console.log('[Recorder] Response status:', response.status);

      if (!response.ok) {
        const errorData = await response.json();
        console.error('[Recorder] Transcription API error:', errorData);
        throw new Error(errorData.error || 'è½¬å½•å¤±è´¥');
      }

      const data = await response.json();
      console.log('[Recorder] Transcription result:', JSON.stringify(data, null, 2));

      if (data.success && data.segments) {
        // æ›´æ–°è½¬å½•ç»“æœ
        const segments: TranscriptSegment[] = data.segments.map((seg: {
          id: string;
          text: string;
          startMs: number;
          endMs: number;
          confidence?: number;
        }) => ({
          id: seg.id,
          text: seg.text,
          startMs: seg.startMs,
          endMs: seg.endMs,
          confidence: seg.confidence || 0.95,
          isFinal: true,
        }));

        setTranscript(segments);
        onTranscriptUpdate?.(segments);
        setTranscribeProgress(`è½¬å½•å®Œæˆï¼Œå…± ${segments.length} ä¸ªå¥å­`);
      } else {
        setTranscribeProgress('è½¬å½•å®Œæˆï¼Œä½†æœªè·å–åˆ°æ–‡æœ¬');
      }
    } catch (err) {
      console.error('[Recorder] Transcription error:', err);
      setError(err instanceof Error ? err.message : 'è½¬å½•å¤±è´¥');
      setTranscribeProgress('');
    } finally {
      onTranscribing?.(false);
      setStatus('stopped');
      onRecordingStop?.(audioBlob);
    }
  };

  // æ ‡è®°æ–­ç‚¹
  const markAnchor = useCallback(() => {
    if (status !== 'recording') return;
    
    const timestamp = elapsedMs;
    lastAnchorTimeRef.current = timestamp;
    onAnchorMark?.(timestamp);
    setCanUndo(true);

    // 5ç§’åå–æ¶ˆæ’¤é”€èƒ½åŠ›
    setTimeout(() => {
      setCanUndo(false);
    }, 5000);
  }, [status, elapsedMs, onAnchorMark]);

  // æ¸…ç†
  useEffect(() => {
    return () => {
      if (animationIdRef.current) {
        cancelAnimationFrame(animationIdRef.current);
      }
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
      if (pcmProcessorRef.current) {
        pcmProcessorRef.current.disconnect();
      }
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop();
        mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (asrClientRef.current) {
        asrClientRef.current.stop();
      }
    };
  }, []);

  return (
    <div className="bg-white rounded-xl shadow-sm border border-gray-200 p-6">
      {/* æœåŠ¡çŠ¶æ€æŒ‡ç¤ºå™¨ */}
      <div className="flex items-center justify-between mb-4">
        <div className="flex items-center gap-2">
          <div className={`w-2 h-2 rounded-full ${
            serviceStatus === 'checking' ? 'bg-yellow-500 animate-pulse' :
            serviceStatus === 'available' ? 'bg-green-500' :
            serviceStatus === 'asr-ready' ? 'bg-blue-500' :
            'bg-gray-400'
          }`} />
          <span className="text-xs text-gray-500">
            {serviceStatus === 'checking' ? 'æ£€æŸ¥æœåŠ¡...' :
             serviceStatus === 'available' ? 'ç™¾ç‚¼ ASR å·²è¿æ¥ï¼ˆæµå¼ï¼‰' :
             serviceStatus === 'asr-ready' ? 'Qwen ASR å°±ç»ª' :
             'æœ¬åœ°å½•éŸ³æ¨¡å¼'}
          </span>
        </div>
        
        {/* è½¬å½•æ¨¡å¼åˆ‡æ¢ */}
        {status === 'idle' && (
          <div className="flex items-center gap-2">
            <span className="text-xs text-gray-500">è½¬å½•æ¨¡å¼ï¼š</span>
            <div className="flex bg-gray-100 rounded-lg p-0.5">
              <button
                onClick={() => setTranscribeMode('batch')}
                className={`px-3 py-1 text-xs rounded-md transition-colors ${
                  transcribeMode === 'batch' 
                    ? 'bg-blue-500 text-white' 
                    : 'text-gray-600 hover:bg-gray-200'
                }`}
              >
                éæµå¼ï¼ˆé«˜ç²¾åº¦ï¼‰
              </button>
              <button
                onClick={() => setTranscribeMode('streaming')}
                disabled={!streamingAvailable}
                className={`px-3 py-1 text-xs rounded-md transition-colors ${
                  transcribeMode === 'streaming' 
                    ? 'bg-green-500 text-white' 
                    : streamingAvailable 
                      ? 'text-gray-600 hover:bg-gray-200' 
                      : 'text-gray-400 cursor-not-allowed'
                }`}
                title={!streamingAvailable ? 'æµå¼æœåŠ¡æš‚ä¸å¯ç”¨ï¼ˆæµè§ˆå™¨é™åˆ¶ï¼‰' : ''}
              >
                æµå¼ï¼ˆå®æ—¶ï¼‰{!streamingAvailable && ' ğŸš«'}
              </button>
            </div>
          </div>
        )}
        
        {/* å½•éŸ³ä¸­æ˜¾ç¤ºå½“å‰æ¨¡å¼ */}
        {status === 'recording' && (
          <span className={`text-xs px-2 py-1 rounded-full ${
            transcribeMode === 'streaming' 
              ? 'bg-green-100 text-green-700' 
              : 'bg-blue-100 text-blue-700'
          }`}>
            {transcribeMode === 'streaming' ? 'æµå¼è½¬å½•ä¸­' : 'å½•éŸ³åè½¬å½•'}
          </span>
        )}
      </div>

      {/* é”™è¯¯æç¤º */}
      {error && (
        <div className="mb-4 p-3 bg-red-50 border border-red-200 rounded-lg text-red-700 text-sm">
          {error}
        </div>
      )}

      {/* è½¬å½•è¿›åº¦æç¤º */}
      {status === 'transcribing' && (
        <div className="mb-4 p-3 bg-blue-50 border border-blue-200 rounded-lg text-blue-700 text-sm flex items-center gap-2">
          <svg className="w-4 h-4 animate-spin" fill="none" viewBox="0 0 24 24">
            <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
            <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
          </svg>
          {transcribeProgress || 'æ­£åœ¨è½¬å½•...'}
        </div>
      )}

      {/* å½•éŸ³çŠ¶æ€ */}
      <div className="flex items-center justify-between mb-6">
        <div className="flex items-center gap-4">
          {/* å½•éŸ³æŒ‡ç¤ºå™¨ */}
          <div className={`w-4 h-4 rounded-full ${
            status === 'recording' ? 'bg-red-500 animate-pulse' :
            status === 'paused' ? 'bg-yellow-500' :
            status === 'transcribing' ? 'bg-blue-500 animate-pulse' :
            status === 'stopped' ? 'bg-gray-400' :
            'bg-gray-300'
          }`} />
          
          {/* æ—¶é—´æ˜¾ç¤º */}
          <span className="text-2xl font-mono font-bold text-gray-900">
            {formatTime(elapsedMs)}
          </span>
        </div>

        {/* éŸ³é‡æŒ‡ç¤ºå™¨ */}
        {status === 'recording' && (
          <div className="flex items-center gap-1">
            {[...Array(10)].map((_, i) => (
              <div
                key={i}
                className={`w-1 rounded-full transition-all ${
                  level * 10 > i ? 'bg-green-500' : 'bg-gray-200'
                }`}
                style={{ height: `${8 + i * 2}px` }}
              />
            ))}
          </div>
        )}
      </div>

      {/* æ§åˆ¶æŒ‰é’® */}
      <div className="flex items-center justify-center gap-4 mb-6">
        {status === 'idle' && (
          <button
            onClick={startRecording}
            disabled={disabled}
            className="flex items-center gap-2 px-6 py-3 bg-red-500 text-white rounded-full hover:bg-red-600 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
          >
            <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
              <circle cx="10" cy="10" r="6" />
            </svg>
            å¼€å§‹å½•éŸ³
          </button>
        )}

        {status === 'recording' && (
          <>
            <button
              onClick={pauseRecording}
              className="flex items-center gap-2 px-4 py-2 bg-yellow-500 text-white rounded-full hover:bg-yellow-600 transition-colors"
            >
              <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                <rect x="5" y="4" width="3" height="12" rx="1" />
                <rect x="12" y="4" width="3" height="12" rx="1" />
              </svg>
              æš‚åœ
            </button>
            <button
              onClick={stopRecording}
              className="flex items-center gap-2 px-4 py-2 bg-gray-500 text-white rounded-full hover:bg-gray-600 transition-colors"
            >
              <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                <rect x="4" y="4" width="12" height="12" rx="2" />
              </svg>
              {transcribeMode === 'batch' ? 'ç»“æŸå¹¶è½¬å½•' : 'ç»“æŸå½•éŸ³'}
            </button>
          </>
        )}

        {status === 'paused' && (
          <>
            <button
              onClick={resumeRecording}
              className="flex items-center gap-2 px-4 py-2 bg-green-500 text-white rounded-full hover:bg-green-600 transition-colors"
            >
              <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                <path d="M6 4l10 6-10 6V4z" />
              </svg>
              ç»§ç»­
            </button>
            <button
              onClick={stopRecording}
              className="flex items-center gap-2 px-4 py-2 bg-gray-500 text-white rounded-full hover:bg-gray-600 transition-colors"
            >
              <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                <rect x="4" y="4" width="12" height="12" rx="2" />
              </svg>
              {transcribeMode === 'batch' ? 'ç»“æŸå¹¶è½¬å½•' : 'ç»“æŸå½•éŸ³'}
            </button>
          </>
        )}

        {status === 'transcribing' && (
          <button
            disabled
            className="flex items-center gap-2 px-6 py-3 bg-blue-400 text-white rounded-full cursor-not-allowed"
          >
            <svg className="w-5 h-5 animate-spin" fill="none" viewBox="0 0 24 24">
              <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
              <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
            </svg>
            è½¬å½•ä¸­...
          </button>
        )}

        {status === 'stopped' && (
          <button
            onClick={() => {
              setStatus('idle');
              setElapsedMs(0);
              setTranscript([]);
              setInterimText('');
              setTranscribeProgress('');
              audioChunksRef.current = [];
            }}
            className="flex items-center gap-2 px-6 py-3 bg-blue-500 text-white rounded-full hover:bg-blue-600 transition-colors"
          >
            <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
              <path fillRule="evenodd" d="M4 2a1 1 0 011 1v2.101a7.002 7.002 0 0111.601 2.566 1 1 0 11-1.885.666A5.002 5.002 0 005.999 7H9a1 1 0 010 2H4a1 1 0 01-1-1V3a1 1 0 011-1zm.008 9.057a1 1 0 011.276.61A5.002 5.002 0 0014.001 13H11a1 1 0 110-2h5a1 1 0 011 1v5a1 1 0 11-2 0v-2.101a7.002 7.002 0 01-11.601-2.566 1 1 0 01.61-1.276z" clipRule="evenodd" />
            </svg>
            æ–°å½•éŸ³
          </button>
        )}
      </div>

      {/* æ–­ç‚¹æ ‡è®°æŒ‰é’® */}
      {(status === 'recording' || status === 'paused') && (
        <div className="border-t border-gray-200 pt-4">
          <button
            onClick={markAnchor}
            disabled={status !== 'recording'}
            className="w-full py-4 bg-gradient-to-r from-orange-500 to-red-500 text-white rounded-xl font-semibold text-lg hover:from-orange-600 hover:to-red-600 disabled:opacity-50 disabled:cursor-not-allowed transition-all transform active:scale-95"
          >
            ğŸ¯ æˆ‘æ²¡å¬æ‡‚è¿™é‡Œ
          </button>
          <p className="text-center text-xs text-gray-400 mt-2">
            {canUndo ? '5ç§’å†…å¯æ’¤é”€' : 'æŒ‰ä¸‹æ ‡è®°å›°æƒ‘ç‚¹'}
          </p>
        </div>
      )}

      {/* å®æ—¶è½¬å½•é¢„è§ˆ */}
      {(transcript.length > 0 || interimText) && (
        <div className="mt-4 border-t border-gray-200 pt-4">
          <h4 className="text-sm font-medium text-gray-700 mb-2 flex items-center gap-2">
            {status === 'transcribing' ? 'è½¬å½•ç»“æœ' : 
             transcribeMode === 'streaming' ? 'å®æ—¶è½¬å½•' : 'è½¬å½•ç»“æœ'}
            {transcribeMode === 'streaming' && (status === 'recording' || transcript.length > 0) && (
              <span className="text-xs px-2 py-0.5 bg-green-100 text-green-700 rounded-full">
                ç™¾ç‚¼ Paraformer
              </span>
            )}
            {transcribeMode === 'batch' && transcript.length > 0 && (
              <span className="text-xs px-2 py-0.5 bg-blue-100 text-blue-700 rounded-full">
                Qwen ASR
              </span>
            )}
          </h4>
          <div className="max-h-48 overflow-y-auto text-sm text-gray-600 bg-gray-50 rounded-lg p-3">
            {transcript.slice(-10).map((seg) => (
              <p key={seg.id} className="mb-1">
                <span className="text-xs text-gray-400 mr-2">
                  {formatTime(seg.startMs)}
                </span>
                {seg.text}
              </p>
            ))}
            {interimText && (
              <p className="mb-1 text-gray-400 italic">
                <span className="text-xs mr-2">...</span>
                {interimText}
              </p>
            )}
          </div>
          {transcript.length > 10 && (
            <p className="text-xs text-gray-400 mt-1 text-center">
              æ˜¾ç¤ºæœ€è¿‘ 10 æ¡ï¼Œå…± {transcript.length} æ¡
            </p>
          )}
        </div>
      )}

      {/* è½¬å½•å®Œæˆæç¤º */}
      {status === 'stopped' && transcribeProgress && (
        <div className="mt-4 p-3 bg-green-50 border border-green-200 rounded-lg text-green-700 text-sm">
          âœ… {transcribeProgress}
        </div>
      )}
    </div>
  );
}
